{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "5de_cqyhN3mM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdnCUY2CNaAt",
        "outputId": "a4d933c7-5a9a-4973-c8e2-c504da40eb6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0        আজ শনিবার ভোররাতে ঢাকার হজরত শাহজালাল আন্তর্জা...\n",
            "1                            কিন্তু আমরা সেটি করতে পারিনি।\n",
            "2                                জীবন চলে তার নিজের গতিতে।\n",
            "3                             চালককে পরে উদ্ধার করে পুলিশ।\n",
            "4        ( ইব্রীয় ১০: ৩৬ - ৩৮) হাত শিথিল করার অথবা শয়তা...\n",
            "                               ...                        \n",
            "69995                   এই ব্যাপারে আমাদের আরো ভাবা জরুরি।\n",
            "69996                                          বেইজ্জত কী?\n",
            "69997    মন্ত্রী জানান, যাঁদের এখন চিকিৎসা চলছে, তাঁদের...\n",
            "69998    এবং কেন নয়, যেহেতু তিনি এর সবেরই অভিজ্ঞতা লাভ ...\n",
            "69999                             কংগ্রেসকে জবাব দিতে হবে।\n",
            "Name: target, Length: 70000, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data=pd.read_csv('task3/team13_bn_train.csv')\n",
        "\n",
        "print(data['target'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "977AQBa9N83G"
      },
      "outputs": [],
      "source": [
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings():\n",
        "  embeddings_index = {}\n",
        "  with open(\"glove.6B/glove.6B.300d.txt\", 'r', encoding='utf-8') as f:\n",
        "      for line in f:\n",
        "          values = line.split()\n",
        "          word = values[0]\n",
        "          vector = np.asarray(values[1:], dtype='float32')\n",
        "          embeddings_index[word] = vector\n",
        "  return embeddings_index\n",
        "\n",
        "# Prepare GloVe embedding matrix\n",
        "def create_embedding_matrix(tokenizer, embedding_dim, embeddings_index):\n",
        "  embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
        "  for word, i in tokenizer.word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "  return embedding_matrix\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "def tokenize_and_pad(texts, tokenizer, max_len):\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  padded = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "  return padded[0]\n",
        "\n",
        "def load_data(filename, with_references=False):\n",
        "  data=pd.read_csv(filename)\n",
        "  en=data.iloc[:, -2].values\n",
        "  bn=data.iloc[:, -1].values\n",
        "\n",
        "  return data,en,bn\n",
        "\n",
        "def indicbert_encode(text, tokenizer, max_beng_len):\n",
        "  input_ids = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=max_beng_len).input_ids\n",
        "  #convert to numpy array\n",
        "  input_ids = input_ids.numpy()[0]\n",
        "  return list(input_ids)\n",
        "\n",
        "def indicbert_embed(text, tokenizer, model, max_beng_len):\n",
        "  input_ids = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=max_beng_len).input_ids\n",
        "  # print(input_ids)\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "  ans = outputs.last_hidden_state.numpy()\n",
        "  #convert into 2d array\n",
        "  ans = ans[0]\n",
        "  # print(ans.shape)\n",
        "  return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "WARj1PgVOAvS"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "# Load training, validation and test data\n",
        "train_data, train_en, train_bn = load_data(\"task3/team13_bn_train.csv\")\n",
        "val_data, val_en, val_bn = load_data(\"task3/team13_bn_valid.csv\")\n",
        "test_data, test_en, test_bn = load_data(\"task3/team13_bn_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ESj-QgPNOMin"
      },
      "outputs": [],
      "source": [
        "# Initialize English tokenizer\n",
        "eng_tokenizer = Tokenizer()\n",
        "eng_tokenizer.fit_on_texts(train_en)\n",
        "\n",
        "# Load GloVe embeddings\n",
        "embeddings_index = load_glove_embeddings()\n",
        "embedding_dim = 300\n",
        "embedding_matrix = create_embedding_matrix(eng_tokenizer, embedding_dim, embeddings_index)\n",
        "\n",
        "# Prepare English data\n",
        "max_eng_len = max(len(seq.split()) for seq in train_en)\n",
        "# encoder_input_data = tokenize_and_pad(train_en, eng_tokenizer, max_eng_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[   1   39   44  111 9807    1  613    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "The police had later apprehended the driver.\n"
          ]
        }
      ],
      "source": [
        "print(tokenize_and_pad([train_en[3]], eng_tokenizer, max_eng_len))\n",
        "print(train_en[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "SE3Zgu5kSkd-"
      },
      "outputs": [],
      "source": [
        "# Initialize Bengali tokenizer using IndicBERT\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')\n",
        "indicbert_model = AutoModel.from_pretrained('ai4bharat/indic-bert')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 1089, 8, 10239, 30032, 23081, 32417, 33209, 8671, 80619, 144742, 34480, 651, 5376, 26080, 2354, 116503, 4083, 17498, 29716, 45428, 17223, 442, 29196, 30032, 442, 3865, 6441, 33727, 46942, 10239, 41417, 2213, 15, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "l = (indicbert_encode(train_bn[0],bert_tokenizer, 220))\n",
        "l1= indicbert_embed(train_bn[0], bert_tokenizer, indicbert_model, 220)\n",
        "# print(tf.shape((l[0])))\n",
        "print(l)\n",
        "# print(indicbert_model.config.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAiu62NdOQEB",
        "outputId": "209ee09c-d591-4017-acf4-9eea25061c01"
      },
      "outputs": [],
      "source": [
        "# Prepare Bengali data\n",
        "max_beng_len = 220 #tested on tokensized bengali data\n",
        "\n",
        "# decoder_input_data = np.zeros((300, max_beng_len, indicbert_model.config.hidden_size))\n",
        "# print(decoder_input_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_data(data):\n",
        "  encoder_input_data = []\n",
        "  decoder_input_data = []\n",
        "  decoder_output_data = []\n",
        "  data_len = len(data)\n",
        "  for i in range(data_len):\n",
        "    eng_sent = data['source'][i]\n",
        "    bn_sent = data['target'][i]\n",
        "    bn_sent_encoded = indicbert_encode(bn_sent, bert_tokenizer, max_beng_len)\n",
        "    if bn_sent_encoded[-1] != 0:\n",
        "      bn_sent_encoded.append(0)\n",
        "    n = bn_sent_encoded.index(0)\n",
        "    for j in range(1,n-1):\n",
        "      encoder_input_data.append(tokenize_and_pad([eng_sent], eng_tokenizer, max_eng_len))\n",
        "      decoder_input_data.append(indicbert_embed([bn_sent[:j]], bert_tokenizer, indicbert_model, max_beng_len))\n",
        "      out_index = bn_sent_encoded[j+1]\n",
        "      #convert it to one hot\n",
        "      # out = np.zeros(indicbert_model.config.vocab_size)\n",
        "      # out[out_index] = 1\n",
        "      decoder_output_data.append(out_index)\n",
        "  encoder_input_data = np.array(encoder_input_data)\n",
        "  decoder_input_data = np.array(decoder_input_data)\n",
        "  decoder_output_data  = keras.utils.to_categorical(decoder_output_data, num_classes=indicbert_model.config.vocab_size)\n",
        "  decoder_output_data = np.array(decoder_output_data)\n",
        "  return encoder_input_data, decoder_input_data, decoder_output_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "P-1fG3aCOeXJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/adityaviraj/Desktop/DL assgn2/.venv/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Model architecture\n",
        "latent_dim = 256\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_eng_len,), name='encoder_inputs')\n",
        "encoder_embedding = Embedding(input_dim=len(eng_tokenizer.word_index) + 1, output_dim=embedding_dim,\n",
        "                              weights=[embedding_matrix], input_length=max_eng_len, trainable=False)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "rF886jfqOhMl"
      },
      "outputs": [],
      "source": [
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_beng_len,indicbert_model.config.hidden_size), name='decoder_inputs')\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=False, return_state=True, name='decoder_lstm')\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(indicbert_model.config.vocab_size, activation='softmax', name='decoder_dense')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To_InAorOjTr",
        "outputId": "b86d5fe4-f777-4402-d630-85ae506c257a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer  [(None, 236)]                0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " embedding_12 (Embedding)    (None, 236, 300)             1360500   ['encoder_inputs[0][0]']      \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer  [(None, 220, 768)]           0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " encoder_lstm (LSTM)         [(None, 256),                570368    ['embedding_12[0][0]']        \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256)]                                                        \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)         [(None, 256),                1049600   ['decoder_inputs[0][0]',      \n",
            "                              (None, 256),                           'encoder_lstm[0][1]',        \n",
            "                              (None, 256)]                           'encoder_lstm[0][2]']        \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)       (None, 200000)               5140000   ['decoder_lstm[0][0]']        \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 66624968 (254.15 MB)\n",
            "Trainable params: 53019968 (202.26 MB)\n",
            "Non-trainable params: 13605000 (51.90 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoYejF2LOngw",
        "outputId": "8b738068-4cc8-47fd-b87c-9ad7bbcd973a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 3s 720ms/step - loss: 4.3721 - accuracy: 0.0545\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 3s 752ms/step - loss: 4.3275 - accuracy: 0.0545\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 3s 728ms/step - loss: 4.3088 - accuracy: 0.0545\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 3s 731ms/step - loss: 4.2989 - accuracy: 0.0545\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 3s 665ms/step - loss: 4.2934 - accuracy: 0.0545\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 3s 775ms/step - loss: 4.2709 - accuracy: 0.0545\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 3s 761ms/step - loss: 4.2682 - accuracy: 0.0545\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 3s 749ms/step - loss: 4.2617 - accuracy: 0.0545\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 3s 699ms/step - loss: 4.2625 - accuracy: 0.0545\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 3s 663ms/step - loss: 4.2502 - accuracy: 0.0545\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x4a985cdd0>"
            ]
          },
          "execution_count": 172,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#generate data\n",
        "encoder_input_data, decoder_input_data, decoder_target_data = gen_data(train_data[:5])\n",
        "# encoder_input_data = np.transpose(encoder_input_data)\n",
        "# encoder_input_data = np.expand_dims(encoder_input_data, axis=0)\n",
        "# encoder_input_data = encoder_input_data.reshape(-1)\n",
        "\n",
        "# Training\n",
        "model.fit([encoder_input_data, decoder_input_data],decoder_target_data, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 107s 3s/step - loss: 5.1291 - accuracy: 0.0693 - val_loss: 6.3450 - val_accuracy: 0.0663\n",
            "36/36 [==============================] - 39s 1s/step - loss: 6.3829 - accuracy: 0.0633 - val_loss: 5.9965 - val_accuracy: 0.0611\n",
            "31/31 [==============================] - 36s 1s/step - loss: 5.8716 - accuracy: 0.0661 - val_loss: 5.7978 - val_accuracy: 0.0847\n",
            "34/34 [==============================] - 39s 1s/step - loss: 5.6658 - accuracy: 0.0684 - val_loss: 5.9067 - val_accuracy: 0.0885\n",
            "33/33 [==============================] - 39s 1s/step - loss: 5.6178 - accuracy: 0.0722 - val_loss: 5.6478 - val_accuracy: 0.0702\n",
            "31/31 [==============================] - 37s 1s/step - loss: 5.5640 - accuracy: 0.0570 - val_loss: 5.5650 - val_accuracy: 0.0697\n",
            "33/33 [==============================] - 39s 1s/step - loss: 5.4733 - accuracy: 0.0853 - val_loss: 6.3754 - val_accuracy: 0.0536\n",
            " 2/32 [>.............................] - ETA: 29s - loss: 5.3420 - accuracy: 0.0859"
          ]
        }
      ],
      "source": [
        "# print(len(train_data))\n",
        "# training data is of size 70000. So we take 100 samples for each batch\n",
        "\n",
        "# Train model\n",
        "for i in range(5): #number of epochs\n",
        "  for i in range(0, len(train_data),100):\n",
        "    df= train_data[i:i+100]\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data = gen_data(df)\n",
        "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=64, epochs=1, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "fLPGsEvWOx8w"
      },
      "outputs": [],
      "source": [
        "# Inference models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_lstm_output, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_lstm_output)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbEz5vmpO3WS"
      },
      "outputs": [],
      "source": [
        "# Translation function\n",
        "def decode_sequence(input_seq):\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "  target_seq = np.zeros((1, 1, indicbert_model.config.hidden_size))\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "\n",
        "  while not stop_condition:\n",
        "      output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "      sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "      sampled_word = bert_tokenizer.decode(sampled_token_index)\n",
        "      decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "      if sampled_word == '[SEP]' or len(decoded_sentence) > max_beng_len:\n",
        "          stop_condition = True\n",
        "\n",
        "      target_seq = np.zeros((1, 1, indicbert_model.config.hidden_size))\n",
        "      target_seq[0, 0, :] = output_tokens[0, -1, :]\n",
        "\n",
        "      states_value = [h, c]\n",
        "\n",
        "  return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "185DKmGOK46r"
      },
      "outputs": [],
      "source": [
        "# BLEU evaluation\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def evaluate_bleu(model, data, eng_tokenizer, bert_tokenizer):\n",
        "  references = []\n",
        "  candidates = []\n",
        "\n",
        "  for i, row in data.iterrows():\n",
        "      input_seq = tokenize_and_pad([row['source']], eng_tokenizer, max_eng_len)\n",
        "      decoded_sentence = decode_sequence(input_seq)\n",
        "      references.append([row['source'].split()])\n",
        "      candidates.append(decoded_sentence.split())\n",
        "\n",
        "  bleu_scores = {\n",
        "      'BLEU-1': corpus_bleu(references, candidates, weights=(1, 0, 0, 0)),\n",
        "      'BLEU-2': corpus_bleu(references, candidates, weights=(0.5, 0.5, 0, 0)),\n",
        "      'BLEU-3': corpus_bleu(references, candidates, weights=(0.33, 0.33, 0.33, 0)),\n",
        "      'BLEU-4': corpus_bleu(references, candidates, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "  }\n",
        "\n",
        "  return bleu_scores\n",
        "\n",
        "# Evaluate on test data\n",
        "test_bleu_scores = evaluate_bleu(model, test_data, eng_tokenizer, bert_tokenizer)\n",
        "print(f\"Test BLEU Scores: {test_bleu_scores}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "print(type(train_data)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "At the instigation of the Corporation of London, city architect Horace Jones proposed a Gothic-style drawbridge to be built downstream from London Bridge. , কর্পোরেশন অভ্ লন্ডন এর অনুরোধে, শহরের স্থপতি হরেস জোনস্ লন্ডন ব্রিজ থেকে নদীর অভিমুখে গথিক ধাঁচে একটা টানা সেতু নির্মাণের প্রস্তাব দিয়েছিলেন।\n",
            "I came here for work. , ইয়ে একটা কাজে তোমার কাছে এসেছিলাম।\n",
            "There are many things. , অনেক বিষয় এসেছে।\n",
            "The garment industry in Bangladesh employs a lot of women. , বাংলাদেশে গার্মেন্ট খাতের শ্রমিকদের সিংহভাগই নারী।\n",
            "The police then fired back at them. , এরপরই কার্যত তাদের উপর ঝাঁপিয়ে পড়ে পুলিস।\n",
            "Sidhu''s wife Navjot Kaur had blamed the chief minister for denial of party ticket to her for Chandigarh seat , সিধুর স্ত্রীকে লোকসভা ভোটের টিকিট না দেওয়ার জন্য অমরিন্দরকেই পাল্টা দোষারোপ করেছিলেন সিধু।\n",
            "\"\"\"They are undergoing treatment at different hospitals.\" , তিনি বলেন, “একটি হাসপাতালে বিভিন্ন ধরনের রোগী থাকে।\n",
            "Paul told Titus, who served congregations in Crete and who appointed overseers, that each appointed elder must be a man free from accusation. , ক্রীত মণ্ডলীর অধ্যক্ষ, তীতকে পৌল বলেছিলেন যে প্রত্যেক প্রাচীনকে “অনিন্দনীয় ” হতে হবে ।\n",
            "Langdon saw it too. , ভিট্টোরিয়ার দেখানো দিকে চোখ পড়েছে ল্যাঙডনেরও।\n",
            "In Manipur, one person died in Imphal West district, official sources said , মনিপুরের পশ্চিম ইম্ফল জেলায় একজনের মৃত্যু হয়েছে।\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "#choose random 10 samples from train data\n",
        "train_data_sample = train_data.sample(n=10)\n",
        "train_data_sample.reset_index(drop=True, inplace=True)\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"{train_data_sample['source'][i]}\",end=\" , \")\n",
        "    print(f\"{train_data_sample['target'][i]}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
